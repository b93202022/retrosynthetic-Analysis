{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:54:57] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\i0947\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading base compounds: 26664423it [00:18, 1410193.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base compounds: 26664423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading base compounds: 9216407it [00:07, 1219121.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base compounds: 32585464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "expansion: 55608it [00:00, 796225.89it/s]\n",
      "rollout: 19728it [00:00, 790360.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\i0947\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [07:36<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synthesis path found. Try increasing `iterations` or `max_depth`.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "#import json\n",
    "#import molvs\n",
    "import random\n",
    "#import policies\n",
    "\n",
    "#from mcts import Node, mcts\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from highway_layer import Highway\n",
    "#匯入深度學習的框架函式庫：keras\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.initializers import Constant\n",
    "from keras.utils import plot_model\n",
    "#keras用以建立模型架構的函數\n",
    "from keras.models import Sequential, load_model, Model\n",
    "\n",
    "#keras中建立深度學習layer的函數\n",
    "\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Multiply, Add, Lambda, Input\n",
    "\n",
    "#keras訓練演算法函數\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#keras提早判停的函數\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#it's hard to reproduce results, so close all seeds\n",
    "#os.environ['PYTHONHASHSEED'] = '0'\n",
    "#np.random.seed(0)\n",
    "#tf.set_random_seed(0)\n",
    "#random.seed(0)\n",
    "\n",
    "#to solve problem:Blas GEMM launch failed\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config)) \n",
    "\n",
    "EXPLORE_PARAM =5.\n",
    "# Monte Carlo tree search numbers: iters\n",
    "iters = 5000\n",
    "# rollout maximum depth: max_d\n",
    "max_d = 5\n",
    "# when node is visited by \"visitn+2\" times, expansions start\n",
    "visitn = 0\n",
    "win = 10.\n",
    "lmax = 25.\n",
    "seed = 0\n",
    "fp_dim_e = 23086\n",
    "fp_dim_r = 8192\n",
    "fp_dim_i = 16384\n",
    "recfp_dim = 2048\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "\n",
    "def wlmax(length, acprob):\n",
    "    ksi = length-0.99*acprob\n",
    "    return max(0,1-ksi/lmax)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None, action=None, is_terminal=False, length=0, prob=0, acprob=0):\n",
    "        self.state = state\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.n_visits = 0\n",
    "        self.reward = 0\n",
    "        self.action = action\n",
    "        self.is_terminal = is_terminal\n",
    "        self.length = length\n",
    "        self.prob = prob\n",
    "        self.acprob = acprob\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"UCB1\"\"\"\n",
    "        if self.n_visits == 0:\n",
    "#            return float('inf')\n",
    "            return self.prob*1e6\n",
    "        return self.reward/self.n_visits + \\\n",
    "            EXPLORE_PARAM*self.prob*math.sqrt(math.log(self.parent.n_visits)/self.n_visits)\n",
    "\n",
    "#    @property\n",
    "    def score(self):    \n",
    "        return wlmax(self.length, self.acprob)\n",
    "\n",
    "    def best_child(self):\n",
    "        return max(self.children, key=lambda n: n.value)\n",
    "\n",
    "\n",
    "def mcts(root, expansion_net, filter_net, rollout_net, iterations=2000, max_depth=200):\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search\n",
    "    - `expansion_policy` should be a function that takes a node and returns a\n",
    "    list of child nodes\n",
    "    - `rollout_policy` should be a function that takes a node and returns a\n",
    "    reward for that node\n",
    "    \"\"\"\n",
    "    pathall=[]\n",
    "    root.children = expansion(root, expansion_net, filter_net)\n",
    "    if not root.children: \n",
    "        print('No synthesis path found. Try adding more data to train model or increasing the rule selection number of `expansion`.')\n",
    "        return None\n",
    "\n",
    "    # MCTS\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        cur_node = root\n",
    "\n",
    "        # Selection\n",
    "        while True:\n",
    "            if cur_node.n_visits >= 0 and cur_node.children:\n",
    "                cur_node = cur_node.best_child()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if cur_node.n_visits > visitn:\n",
    "            # If selection took us to a terminal node,\n",
    "            # this seems to be the best path\n",
    "            if cur_node.is_terminal:\n",
    "                # Return best path\n",
    "                cur_node1 = root\n",
    "                path1 = [cur_node1]\n",
    "                #for _ in range(lmax*10):\n",
    "                while True:\n",
    "                    #if not cur_node.children: continue \n",
    "                    cur_node1 = cur_node1.best_child()\n",
    "                    path1.append(cur_node1)\n",
    "                    if cur_node1.is_terminal:\n",
    "                        break      \n",
    "                if path1 not in pathall:\n",
    "                    pathall.append(path1)\n",
    "                    \n",
    "                # Update, reward-1 to avoid repeated process\n",
    "                #cur_node.reward += win*wlmax(cur_node.length, cur_node.acprob)\n",
    "                cur_node.reward += -win*(visitn+1)*wlmax(cur_node.length, cur_node.acprob)\n",
    "                #cur_node.reward += -win*(cur_node.n_visits+1)*wlmax(cur_node.length, cur_node.acprob)\n",
    "                cur_node.n_visits += 1\n",
    "                parent = cur_node.parent\n",
    "                while parent is not None:\n",
    "                    #parent.reward += -win*wlmax(cur_node.length, parent.acprob)\n",
    "                    #parent.reward += win*wlmax(cur_node.length, cur_node.acprob)\n",
    "                    parent.reward += -win*(visitn+1)*wlmax(cur_node.length, cur_node.acprob)\n",
    "                    #parent.reward += -1*(parent.n_visits+1)*wlmax(cur_node.length, cur_node.acprob)\n",
    "                    parent.n_visits += 1\n",
    "                    parent = parent.parent\n",
    "                #print('test')\n",
    "                #return pathall\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Expansion\n",
    "            s = time()\n",
    "            cur_node.children = expansion(cur_node, expansion_net, filter_net)\n",
    "#            print('Expansion took:', time() - s)\n",
    "            if not cur_node.children:\n",
    "                # Update\n",
    "                cur_node.reward += -1*wlmax(cur_node.length, cur_node.acprob)\n",
    "                cur_node.n_visits += 1\n",
    "                parent = cur_node.parent\n",
    "                while parent is not None:\n",
    "                    #parent.reward += -1*wlmax(cur_node.length, parent.acprob)\n",
    "                    parent.reward += -1*wlmax(cur_node.length, cur_node.acprob)\n",
    "                    parent.n_visits += 1\n",
    "                    parent = parent.parent\n",
    "                continue\n",
    "                \n",
    "            cur_node = cur_node.best_child()\n",
    "\n",
    "        # Rollout\n",
    "        s = time()\n",
    "        reward, length, acprob = rollout(cur_node, rollout_net, max_depth=max_depth)\n",
    "#        print('Rollout took:', time() - s)\n",
    "\n",
    "        # Update\n",
    "        #cur_node.reward += reward*wlmax(length, cur_node.acprob)\n",
    "        cur_node.reward += reward*wlmax(length, acprob)\n",
    "        cur_node.n_visits += 1\n",
    "        parent = cur_node.parent\n",
    "        while parent is not None:\n",
    "            #parent.reward += reward*wlmax(length, parent.acprob)\n",
    "            parent.reward += reward*wlmax(length, acprob)\n",
    "            parent.n_visits += 1\n",
    "            parent = parent.parent\n",
    "\n",
    "#    # Return best path\n",
    "#    cur_node = root\n",
    "#    path = [cur_node]\n",
    "#    for _ in range(lmax*10):\n",
    "#        if not cur_node.children: continue \n",
    "#        cur_node = cur_node.best_child()\n",
    "#        path.append(cur_node)\n",
    "#        if cur_node.is_terminal:\n",
    "#            break\n",
    "\n",
    "#    # Max depth exceeded, no path found\n",
    "#    else:\n",
    "#        return None\n",
    "\n",
    "#    return path\n",
    "\n",
    "    # Max depth exceeded, no path found\n",
    "    #if not pathall: return None\n",
    "    return pathall\n",
    "\n",
    "def fps_to_arr_r(fps):\n",
    "    \"\"\"Faster conversion to ndarray\"\"\"\n",
    "    arrs = []\n",
    "    for fp in fps:\n",
    "        onbits = list(fp.GetOnBits())\n",
    "        arr = np.zeros(fp.GetNumBits())\n",
    "        arr[onbits] = 1\n",
    "        arrs.append(arr)\n",
    "    arrs = np.array(arrs)\n",
    "    return arrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fingerprint_mols_r(mols, fp_dim):\n",
    "    fps = []\n",
    "    for mol in mols:\n",
    "        mol = Chem.MolFromSmiles(mol)\n",
    "\n",
    "        # Necessary for fingerprinting\n",
    "        # Chem.GetSymmSSSR(mol)\n",
    "\n",
    "        # \"When comparing the ECFP/FCFP fingerprints and\n",
    "        # the Morgan fingerprints generated by the RDKit,\n",
    "        # remember that the 4 in ECFP4 corresponds to the\n",
    "        # diameter of the atom environments considered,\n",
    "        # while the Morgan fingerprints take a radius parameter.\n",
    "        # So the examples above, with radius=2, are roughly\n",
    "        # equivalent to ECFP4 and FCFP4.\"\n",
    "        # <http://www.rdkit.org/docs/GettingStartedInPython.html>\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=int(fp_dim), useChirality=1)\n",
    "        # fold_factor = fp.GetNumBits()//fp_dim\n",
    "        # fp = DataStructs.FoldFingerprint(fp, fold_factor)\n",
    "        fps.append(fp)\n",
    "    return fps\n",
    "\n",
    "def fps_to_arr(fps):\n",
    "    \"\"\"Faster conversion to ndarray\"\"\"\n",
    "    arrs = []\n",
    "    for fp, info in zip(fps[0],fps[1]):\n",
    "        onbits = list(fp.GetOnBits())\n",
    "        arr = np.zeros(fp.GetNumBits())\n",
    "        for onbit in onbits:\n",
    "            arr[onbit] = len(info[onbit])\n",
    "        arrs.append(arr)\n",
    "    arrs = np.array(arrs)\n",
    "    return arrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fingerprint_mols(mols, fp_dim):\n",
    "    fps = []\n",
    "    infos = []\n",
    "    for mol in mols:\n",
    "        mol = Chem.MolFromSmiles(mol)\n",
    "        info={}\n",
    "        # Necessary for fingerprinting\n",
    "        # Chem.GetSymmSSSR(mol)\n",
    "\n",
    "        # \"When comparing the ECFP/FCFP fingerprints and\n",
    "        # the Morgan fingerprints generated by the RDKit,\n",
    "        # remember that the 4 in ECFP4 corresponds to the\n",
    "        # diameter of the atom environments considered,\n",
    "        # while the Morgan fingerprints take a radius parameter.\n",
    "        # So the examples above, with radius=2, are roughly\n",
    "        # equivalent to ECFP4 and FCFP4.\"\n",
    "        # <http://www.rdkit.org/docs/GettingStartedInPython.html>\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=int(fp_dim), useChirality=1, bitInfo=info)\n",
    "        # fold_factor = fp.GetNumBits()//fp_dim\n",
    "        # fp = DataStructs.FoldFingerprint(fp, fold_factor)\n",
    "        fps.append(fp)\n",
    "        infos.append(info)\n",
    "    return fps, infos\n",
    "\n",
    "def preprocess_e(X, fp_dim):\n",
    "    # Compute fingerprints\n",
    "    dataX = fps_to_arr(fingerprint_mols(X, fp_dim))\n",
    "    # Apply variance threshold\n",
    "    # return np.log(X[:,self.idx] + 1) \n",
    "    #FPs = np.log(dataX[:,idx]+1)\n",
    "    FPs = np.log(dataX+1)\n",
    "    return FPs\n",
    "\n",
    "def preprocess_r(X,fp_dim):\n",
    "    # Compute fingerprints\n",
    "    dataX = fps_to_arr_r(fingerprint_mols_r(X, fp_dim))\n",
    "    FPs = np.log(dataX+1)\n",
    "    return FPs\n",
    "\n",
    "def preprocess_i(X, fp_dim):\n",
    "    # Compute fingerprints\n",
    "    FPs = fps_to_arr(fingerprint_mols(X, fp_dim))\n",
    "    # Apply variance threshold\n",
    "    # return np.log(X[:,self.idx] + 1) \n",
    "    #FPs = np.log(dataX[:,idx]+1)\n",
    "#    FPs = np.log(dataX+1)\n",
    "    return FPs\n",
    "def smi_list_from_str(inchis):\n",
    "    '''string separated by ++ to list of RDKit molecules'''\n",
    "    return [inchi.strip() for inchi in inchis.split('++')]\n",
    "\n",
    "def acc_top50(y_true, y_pred):\n",
    "    return keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=50)\n",
    "\n",
    "def acc_top10(y_true, y_pred):\n",
    "    return keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=10)\n",
    "\n",
    "def fold(x):\n",
    "    z=tf.subtract(x[0], x[1])\n",
    "#    z_shape=tf.Tensor.shape(z)\n",
    "\n",
    "#    z_shape=z.get_shape().as_list()\n",
    "    zv=tf.reshape(z,[-1,8,2048])\n",
    "    return tf.reduce_sum(zv, 1) \n",
    "\n",
    "def cosine(x):\n",
    "    prod_net = x[0]\n",
    "    react_net = x[1]\n",
    "#    prod_norm = tf.nn.l2_normalize(prod_net, axis=-1)\n",
    "#    react_norm = tf.nn.l2_normalize(react_net, axis=-1)\n",
    "    cosine_sim = tf.reduce_sum(tf.multiply(prod_net, react_net), axis=-1,keepdims=True)\n",
    "#    cosine_sim = tf.squeeze(cosine_sim,[1])\n",
    "#    return tf.nn.sigmoid(cosine_sim)\n",
    "    return tf.nn.sigmoid(cosine_sim)\n",
    "# get average auc between different batches over the epoch, so don't use. otherwise validation process always get wrong results\n",
    "def auc2(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "# AUC for a binary classifier, this AUC is a little underestimated due to minimum areas.\n",
    "def auc1(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier(FPR)\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier(TPR)\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P\n",
    "# PFA, prob false alert for binary classifier(FPR)\n",
    "def FPR(y_true, y_pred):\n",
    "    y_pred = K.cast(y_pred >= 0.9, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier(TPR)\n",
    "def TPR(y_true, y_pred):\n",
    "    y_pred = K.cast(y_pred >= 0.9, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P\n",
    "\n",
    "# ACC= (TP + TN) / (P + N)\n",
    "def ACCR(y_true, y_pred):\n",
    "    y_pred = K.cast(y_pred >= 0.9, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)    \n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    # TN = total number of correct alerts, alerts from the negtive class labels\n",
    "    TN = K.sum((1-y_pred) * (1-y_true))    \n",
    "    return (TP+TN)/(P+N)\n",
    "\n",
    "    \n",
    "# Load base compounds\n",
    "starting_mols = set()\n",
    "expansion_rules = []\n",
    "rollout_rules = []\n",
    "#with open('data/emolecules.smi', 'r') as f:\n",
    "#    for line in tqdm(f, desc='Loading base compounds'):\n",
    "#        smi = line.strip()\n",
    "###        smi = molvs.standardize_smiles(smi)\n",
    "#        smi = Chem.MolFromSmiles(smi)\n",
    "#        if not smi: continue\n",
    "#        smi = Chem.MolToSmiles(smi,allHsExplicit=0,allBondsExplicit=0)\n",
    "#        starting_mols.add(smi)\n",
    "#with open('data/emoleculestandard.dat', 'w') as f:\n",
    "#    f.write('\\n'.join(starting_mols))  \n",
    "\n",
    "#'''    \n",
    "#with open('data/emoleculestandard.dat', 'r') as f:\n",
    "\n",
    "with open('data/emoleculestandard0701.dat', 'r') as f:\n",
    "    for line in tqdm(f, desc='Loading base compounds'):\n",
    "        smi = line.strip()\n",
    "#        smi = molvs.standardize_smiles(smi)\n",
    "#        smi = Chem.MolFromSmiles(smi)\n",
    "#        if not smi: continue\n",
    "#        smi = Chem.MolToSmiles(smi,allHsExplicit=0,allBondsExplicit=0)\n",
    "        starting_mols.add(smi)\n",
    "        \n",
    "print('Base compounds:', len(starting_mols))\n",
    "#'''\n",
    "#'''    \n",
    "\n",
    "\n",
    "with open('data/zincagent.dat', 'r') as f:\n",
    "    for line in tqdm(f, desc='Loading base compounds'):\n",
    "        smi = line.strip()\n",
    "#        smi = molvs.standardize_smiles(smi)\n",
    "#        smi = Chem.MolFromSmiles(smi)\n",
    "#        if not smi: continue\n",
    "#        smi = Chem.MolToSmiles(smi,allHsExplicit=0,allBondsExplicit=0)\n",
    "        starting_mols.add(smi)\n",
    "        \n",
    "print('Base compounds:', len(starting_mols))\n",
    "#'''\n",
    "\n",
    "'''\n",
    "start=time()\n",
    "with open('data/emoleculestandard.pickle', 'rb') as f:\n",
    "    starting_mols = pickle.load(f)\n",
    "print(time()-start, 's')        \n",
    "print('Base compounds:', len(starting_mols))\n",
    "'''\n",
    "\n",
    "# Load policy networks\n",
    "#with open('model/rules.json', 'r') as f:\n",
    "#    rules = json.load(f)\n",
    "#    rollout_rules = rules['rollout']\n",
    "#    expansion_rules = rules['expansion']\n",
    "    \n",
    "with open('data/expansion_expansion.dat', 'r') as f:\n",
    "    for i, l in tqdm(enumerate(f), desc='expansion'):\n",
    "        rule = l.strip()\n",
    "        expansion_rules.append(rule)\n",
    "with open('data/rollout_rollout.dat', 'r') as f:\n",
    "    for i, l in tqdm(enumerate(f), desc='rollout'):\n",
    "        rule = l.strip()\n",
    "        rollout_rules.append(rule)\n",
    "        \n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path_r = os.path.join(save_dir, 'trained_model_rollout_2')\n",
    "model_path_e = os.path.join(save_dir, 'trained_model_expansion_4')\n",
    "model_path_i = os.path.join(save_dir, 'trained_model_inscope_18')\n",
    "threshold = 0.5\n",
    "\n",
    "rollout_net = load_model(model_path_r, custom_objects={'acc_top10': acc_top10,'acc_top50': acc_top50})\n",
    "expansion_net = load_model(model_path_e, custom_objects={'acc_top10': acc_top10,'acc_top50': acc_top50, 'Highway': Highway})\n",
    "filter_net =  load_model(model_path_i, custom_objects={'ACCR': ACCR,'auc2': auc2,'auc1': auc1,'TPR': TPR, 'FPR': FPR,'Highway': Highway,'fold': fold,'cosine': cosine, 'tf': tf})\n",
    "\n",
    "def convert_to_retro(transform):\n",
    "    '''This function takes a forward synthesis and converts it to a\n",
    "    retrosynthesis. Only transforms with a single product are kept, since\n",
    "    retrosyntheses should have a single reactant (and split it up accordingly).'''\n",
    "\n",
    "    # Split up original transform\n",
    "    reactants = transform.split('>>')[0]\n",
    "    products  = transform.split('>>')[1]\n",
    "\n",
    "    # Don't force products to be from different molecules (?)\n",
    "    # -> any reaction template can be intramolecular (might remove later)\n",
    "    #products = products[1:-1].replace(').(', '.')\n",
    "\n",
    "    # Don't force the \"products\" of a retrosynthesis to be two different molecules!\n",
    "    #reactants = reactants[1:-1].replace(').(', '.')\n",
    "\n",
    "    return '>>'.join([products, reactants])\n",
    "\n",
    "def transform(mol, rule, mode='exp'):\n",
    "    \"\"\"Apply transformation rule to a molecule to get reactants\"\"\"\n",
    "    reactants = []\n",
    "    reactants_smi = []\n",
    "    results = []\n",
    "    rxn = AllChem.ReactionFromSmarts(rule)\n",
    "    #the below should be no isomerics for comparing with original molecule, because all rules I use cannot distinguish whether the product is isometrc or not \n",
    "    if mode == 'exp':\n",
    "        mol_smi = Chem.MolToSmiles(mol,allHsExplicit=0,allBondsExplicit=0, isomericSmiles=0)\n",
    "    try:\n",
    "        results = rxn.RunReactants([mol])\n",
    "    except Exception as e:\n",
    "        print('error: {}'.format(e))\n",
    "        print('rxn: {}'.format(rule))    \n",
    "    \n",
    "    if not results: return []    \n",
    "    for result in results:\n",
    "        mols = []\n",
    "        mols_obj= []\n",
    "        for i,mol in enumerate(result):\n",
    "            repeat = 0\n",
    "            try: \n",
    "                #To save time, so i cancell the following step in the below\n",
    "                Chem.SanitizeMol(mol)\n",
    "#                product.UpdatePropertyCache()\n",
    "                #create product or reactant using molfromsmarts+sanitizemol is sometimes better than molfromsmiles, but still using molfromsmiles as possible as you can\n",
    "                #To save time, so i cancell the following step in the below\n",
    "                #mol=Chem.MolFromSmiles(Chem.MolToSmiles(mol,allHsExplicit=0,allBondsExplicit=0))\n",
    "            except Exception as e:\n",
    "                #print('warning1: {}'.format(e))\n",
    "                #use pass is not good behavior, however i have validation finally\n",
    "                break\n",
    "            if not mol:\n",
    "                break\n",
    "            a = Chem.MolToSmiles(mol,allHsExplicit=0,allBondsExplicit=0, isomericSmiles=1)\n",
    "            if '.' in a: break\n",
    "            '''    \n",
    "            if reactants and i == 0:\n",
    "                for reac in reactants:\n",
    "                    if a in reac:\n",
    "                        repeat = 1\n",
    "                        break\n",
    "            if repeat: break\n",
    "            '''\n",
    "            mols.append(a)\n",
    "            mols_obj.append(mol)\n",
    "            if i == len(result)-1 and mode == 'exp':\n",
    "                retrorule = convert_to_retro(rule)\n",
    "                retrorxn = AllChem.ReactionFromSmarts(retrorule)\n",
    "    \n",
    "                try:\n",
    "                    retroresults = retrorxn.RunReactants(mols_obj)\n",
    "                except Exception as e:\n",
    "                    print('error retro: {}'.format(e))\n",
    "                    print('rxn retro: {}'.format(retrorule))    \n",
    "\n",
    "                if not retroresults: break \n",
    "                test_mol = []    \n",
    "                for result in retroresults:\n",
    "                    \n",
    "                    for i,mol in enumerate(result):\n",
    "            \n",
    "                        try: \n",
    "                            #To save time, so i cancell the following step in the below\n",
    "                            Chem.SanitizeMol(mol)\n",
    "            #                product.UpdatePropertyCache()\n",
    "                            #create product or reactant using molfromsmarts+sanitizemol is sometimes better than molfromsmiles, but still using molfromsmiles as possible as you can\n",
    "                            #To save time, so i cancell the following step in the below\n",
    "                            #mol=Chem.MolFromSmiles(Chem.MolToSmiles(mol,allHsExplicit=0,allBondsExplicit=0))\n",
    "                        except Exception as e:\n",
    "                            #print('warning1: {}'.format(e))\n",
    "                            #use pass is not good behavior, however i have validation finally\n",
    "                            break\n",
    "                        if not mol:\n",
    "                            break\n",
    "                        #the below should be no isomerics for comparing with original molecule, because all rules I use cannot distinguish whether the product is isometrc or not \n",
    "                        b = Chem.MolToSmiles(mol,allHsExplicit=0,allBondsExplicit=0, isomericSmiles=0)\n",
    "                        test_mol.append(b)\n",
    "                if not test_mol: break \n",
    "                if mol_smi not in test_mol: break\n",
    "                \n",
    "        else:\n",
    "            reactants_smi_one = '++'.join(mols)\n",
    "            if reactants_smi_one not in reactants_smi: \n",
    "                reactants_smi.append(reactants_smi_one)\n",
    "                reactants.append(mols)\n",
    "            \n",
    "    # Only look at first set of results (TODO any reason not to?)\n",
    "    #results = results[0]\n",
    "    #reactants = [Chem.MolToSmiles(smi) for smi in results]\n",
    "    return reactants\n",
    "\n",
    "\n",
    "def expansion(node, expansion_net, filter_net):\n",
    "    \"\"\"Try expanding each molecule in the current state\n",
    "    to possible reactants\"\"\"\n",
    "\n",
    "    # Assume each mol is a SMILES string\n",
    "    mols = node.state\n",
    "\n",
    "    # Convert mols to format for prediction\n",
    "    # If the mol is in the starting set, ignore\n",
    "    mols = [mol for mol in mols if mol not in starting_mols]\n",
    "    e_x = preprocess_e(mols, fp_dim_e)\n",
    "\n",
    "    # Predict applicable rules\n",
    "    predict = expansion_net.predict_on_batch(e_x)\n",
    "    # get the rules index from high probability to low probability\n",
    "    preds =np.argsort(-predict, axis=1)[:,:50]\n",
    "    \n",
    "\n",
    "\n",
    "    # Generate children for reactants\n",
    "    children = []\n",
    "    count = -1\n",
    "    for mol, rule_idxs in zip(mols, preds):\n",
    "        count += 1\n",
    "        # State for children will\n",
    "        # not include this mol\n",
    "        new_state = node.state - {mol}\n",
    "\n",
    "        prod = Chem.MolFromSmiles(mol)\n",
    "        if not prod: return []       \n",
    "        for idx in rule_idxs:\n",
    "            # Extract actual rule\n",
    "            rule = expansion_rules[idx]\n",
    "\n",
    "            # TODO filter_net should check if the reaction will work?\n",
    "            # should do as a batch\n",
    "\n",
    "            # Apply rule\n",
    "            reactants = transform(prod, rule)\n",
    "            #, mode='exp'\n",
    "            if not reactants: continue\n",
    "#            reactants = list(reactant1) \n",
    "            X = np.zeros((len(reactants), fp_dim_i))\n",
    "            X[0] = preprocess_i([mol], fp_dim_i)                \n",
    "            for i, reactant in enumerate(reactants):\n",
    "                X[i] = X[0]\n",
    "                y = np.zeros((len(reactants),recfp_dim))\n",
    "                \n",
    "                n = np.zeros((1,recfp_dim))\n",
    "                for b in reactant:\n",
    "                    n += preprocess_i([b], recfp_dim)\n",
    "                p = X[i].reshape((-1, recfp_dim))    \n",
    "                y[i] = np.sum(p, 0, keepdims=True)- n \n",
    "            # Predict applicable rules\n",
    "            predict_i = filter_net.predict_on_batch([X, y])\n",
    "\n",
    "                \n",
    "            Treactants = [reactant for reactant, pred in zip(reactants, predict_i) if pred >= threshold]\n",
    "            Tpreds = [pred for reactant, pred in zip(reactants, predict_i) if pred >= threshold]\n",
    "            if not Treactants: continue\n",
    "            for reactant, pred in zip(Treactants, Tpreds):\n",
    "                state = new_state | set(reactant)\n",
    "                terminal = all(mol in starting_mols for mol in state)\n",
    "                #child = Node(state=state, is_terminal=terminal, parent=node, action=rule, length=node.length+1, prob= predict[count,idx], acprob=node.acprob+predict[count,idx])\n",
    "                child = Node(state=state, is_terminal=terminal, parent=node, action=rule, length=node.length+1, prob= pred[0]/30, acprob=node.acprob+pred[0]/30)\n",
    "                children.append(child)\n",
    "    return children\n",
    "\n",
    "\n",
    "def rollout(node, rollout_net, max_depth=30):\n",
    "    cur = node\n",
    "    ## only focus on unsolved molecules to avoid getting good reward but unsolved molecules appear\n",
    "    state = {mol for mol in cur.state if mol not in starting_mols}\n",
    "    cur = Node(state=state, is_terminal=cur.is_terminal, parent=cur.parent, action=cur.action, length=cur.length, prob= cur.prob, acprob=cur.acprob)\n",
    "    for _ in range(max_depth):\n",
    "        if cur.is_terminal:\n",
    "            break\n",
    "\n",
    "        # Select a random mol (that's not a starting mol)\n",
    "        mols = [mol for mol in cur.state if mol not in starting_mols]\n",
    "        mol = random.choice(mols)\n",
    "        prod = Chem.MolFromSmiles(mol)\n",
    "#        if not prod: return -1., cur.length \n",
    "        if not prod:\n",
    "            continue\n",
    "            '''\n",
    "            # Partial reward if some starting molecules are found\n",
    "            reward = sum(1 for mol in cur.state if mol in starting_mols)/len(cur.state)\n",
    "\n",
    "            # Reward of -1 if no starting molecules are found\n",
    "            if reward == 0:\n",
    "                return -1., cur.length, cur.acprob\n",
    "\n",
    "            return reward, cur.length, cur.acprob\n",
    "            '''\n",
    "        \n",
    "        r_x = preprocess_r([mol], fp_dim_r)\n",
    "\n",
    "        # Predict applicable rules\n",
    "        predict = rollout_net.predict_on_batch(r_x)\n",
    "        # get the rules index from high probability to low probability\n",
    "        preds =np.argsort(-predict, axis=1)[:,:10]\n",
    "        idx = np.random.choice(preds[0])\n",
    "        rule = rollout_rules[idx]\n",
    "        \n",
    "        reactants = transform(prod, rule, mode='rol')\n",
    "        #, mode='rol'\n",
    "        if not reactants:\n",
    "            continue\n",
    "            '''\n",
    "            # Partial reward if some starting molecules are found\n",
    "            reward = sum(1 for mol in cur.state if mol in starting_mols)/len(cur.state)\n",
    "\n",
    "            # Reward of -1 if no starting molecules are found\n",
    "            if reward == 0:\n",
    "                return -1., cur.length, cur.acprob\n",
    "\n",
    "            return reward, cur.length, cur.acprob\n",
    "            '''\n",
    "#        reactants = list(reactant1)\n",
    "        reactant = random.choice(reactants)\n",
    "        \n",
    "            \n",
    "        state = cur.state | set(reactant)\n",
    "\n",
    "        # State for children will\n",
    "        # not include this mol\n",
    "        state = state - {mol}\n",
    "\n",
    "        terminal = all(mol in starting_mols for mol in state)\n",
    "        cur = Node(state=state, is_terminal=terminal, parent=cur, action=rule, length=cur.length+1, prob= predict[0,idx], acprob=cur.acprob+predict[0,idx])\n",
    "\n",
    "    # Max depth exceeded\n",
    "    else:\n",
    "        #print('Rollout reached max depth')\n",
    "\n",
    "        # Partial reward if some starting molecules are found\n",
    "        reward = sum(1 for mol in cur.state if mol in starting_mols)/len(cur.state)\n",
    "\n",
    "        # Reward of -1 if no starting molecules are found\n",
    "        if reward == 0:\n",
    "            return -1., cur.length, cur.acprob\n",
    "        if reward == 1:\n",
    "            return win , cur.length, cur.acprob        \n",
    "\n",
    "        return reward, cur.length, cur.acprob\n",
    "\n",
    "    # Reward of 1 if solution is found\n",
    "    return win , cur.length, cur.acprob\n",
    "\n",
    "\n",
    "def plan(target_mol, expansion_net, filter_net, rollout_net, iterations=2000, max_depth=200):\n",
    "    \"\"\"Generate a synthesis plan for a target molecule (in SMILES form).\n",
    "    If a path is found, returns a list of (action, state) tuples.\n",
    "    If a path is not found, returns None.\"\"\"\n",
    "    root = Node(state={target_mol})\n",
    "    pathall = []\n",
    "    path = mcts(root, expansion_net, filter_net, rollout_net, iterations=iterations, max_depth=max_depth)\n",
    "    if not path:\n",
    "        print('No synthesis path found. Try increasing `iterations` or `max_depth`.')\n",
    "    else:\n",
    "        print('Path found:')\n",
    "        #path = [(n.action, n.state) for n in path[1:]]\n",
    "        for i in path:\n",
    "            #ii = [(n.action, n.state, n.reward/n.n_visits) for n in i[1:]]\n",
    "            ii = [(n.action, n.state) for n in i[1:]]\n",
    "            ii.append(i[-1].score())\n",
    "            pathall.append(ii)\n",
    "    return pathall\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Tropantiol-TRODAT-1:CN1[C@H]2CC[C@@H]1[C@H]([C@H](C2)C3=CC=C(C=C3)Cl)CN(CCNCCS)CCS\n",
    "    #target_mol1 = 'CN1C2CCC1C(C(C2)C3=CC=C(C=C3)Cl)CN(CCNCCS)CCS'\n",
    "    #target_mol1 = 'CN1[C@H]2CC[C@@H]1[C@H]([C@H](C2)C3=CC=C(C=C3)Cl)CN(CCNCCS)CCS'\n",
    "    m1= 'COC(=O)[C@H]1[C@@H](c2ccc(Cl)cc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    m2= 'COC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C'\n",
    "    m3= 'CCOC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C'\n",
    "    m4= 'COC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    m5= 'CCOC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    m6= 'CN1[C@H]2CC[C@@H]1[C@@H](C(=O)O)[C@@H](O)C2'\n",
    "    m7= 'CN1[C@H]2CC[C@@H]1[C@@H](C(=O)O)[C@@H](OC(=O)c1ccccc1)C2'\n",
    "    m8= 'CC(C)OC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    m9= 'CN1[C@H]2CC[C@@H]1CC(=O)C2'\n",
    "    m10= 'CN1[C@H]2CC[C@@H]1[C@@H](COC(=O)c1ccccc1)[C@@H](O)C2'\n",
    "    m11='COC(=O)[C@H]1CC[C@@H]2CC[C@H]1N2'\n",
    "    m12='CN1[C@@H]2CC=C(C(=O)O)[C@H]1CC2'\n",
    "    m13='CN1[C@H]2CC[C@@H]1[C@@H](C(=O)O)[C@@H](OC(=O)c1ccc(O)cc1)C2'\n",
    "    m14='CCOC(=O)C1=CC[C@@H]2CC[C@H]1N2C'\n",
    "    m15='COC(=O)[C@H]1[C@@H](c2ccccc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    m16='CN1[C@H]2CC[C@@H]1[C@@H](C(=O)Oc1ccccc1)[C@@H](c1ccc(Cl)cc1)C2'\n",
    "    m17='CCC(CC)COC(=O)[C@H](C)N[P@](=O)(OC[C@H]1O[C@@](C#N)(c2ccc3c(N)ncnn23)[C@H](O)[C@@H]1O)Oc1ccccc1' # REMDESIVIR\n",
    "    m18='N#C[C@@]1(c2ccc3c(N)ncnn23)O[C@H](CO)[C@@H](O)[C@H]1O' # initiative of REMDESIVIR \n",
    "    m19='N#C[C@@]1(c2ccc3c(N)ncnn23)O[C@H](COCc2ccccc2)[C@@H](OCc2ccccc2)[C@H]1OCc1ccccc1'# initiative of REMDESIVIR\n",
    "    #del_mol1 = [m1,m15,m16]\n",
    "    del_mol1 = [m1,m2,m3,m4,m5,m6,m7,m8,m9 ,m10,m11,m12,m13,m14,m17,m18,m19]\n",
    "    starting_mols.difference_update(del_mol1)\n",
    "    #del_mol2 = 'COC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C'\n",
    "    #starting_mols.remove(del_mol2)\n",
    "    \n",
    "    # new initiative reactant for trodat1\n",
    "    #target_mol1 = 'COC(=O)[C@H]1[C@@H](c2ccc(Cl)cc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    #target_mol1 = 'COC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C'\n",
    "    #target_mol1 = 'CN1[C@H]2CC[C@@H]1[C@@H](C(=O)Oc1ccccc1)[C@@H](c1ccc(Cl)cc1)C2'\n",
    "    #target_mol1 = 'COC(=O)[C@H]1[C@@H](c2ccccc2)C[C@@H]2CC[C@H]1N2C'\n",
    "    #del_mol1 = ['COC(=O)[C@H]1[C@@H](c2ccc(Cl)cc2)C[C@@H]2CC[C@H]1N2C']\n",
    "    #starting_mols.difference_update(del_mol1)\n",
    "    \n",
    "    # new initiative reactant for trodat1\n",
    "    #target_mol1 = 'COC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C'\n",
    "    #del_mol1 = ['COC(=O)[C@H]1[C@@H](O)C[C@@H]2CC[C@H]1N2C']\n",
    "    #starting_mols.difference_update(del_mol1)\n",
    "    \n",
    "    #below is 同位素\n",
    "    #target_mol1 ='FCCn4ccc(c3cnc(n2ccc1ccncc12)nc3)n4'\n",
    "    #target_mol1 ='CNc1ccc(/C=C/c2ccc(OCCCCCCF)nc2)cc1'\n",
    "    #target_mol1 ='CNc3ccc(C2Nc1ccc(O)cc1S2)cc3'\n",
    "    \n",
    "    #target_mol1 ='Nc3cccc4cnc(n2ccc1ccncc12)cc34'\n",
    "    #del_mol1 = 'Nc3cccc4cnc(n2ccc1ccncc12)cc34'\n",
    "    #del_mol1 = [Chem.MolToSmiles(Chem.MolFromSmiles(del_mol1),allHsExplicit=0,allBondsExplicit=0, isomericSmiles=1)]\n",
    "    #starting_mols.difference_update(del_mol1)\n",
    "\n",
    "    #target_mol1 ='OCCC4CCN(c3ccc(n2ccc1ccncc12)nc3)CC4'\n",
    "    #target_mol1 ='NNc1ccc(/C=C/c2ccc(OCCCCCCF)nc2)cc1'\n",
    "    #target_mol1 ='FNc3ccc(C2Nc1ccc(O)cc1S2)cc3'\n",
    "    #target_mol1 ='Fc3cccc4cnc(n2ccc1ccncc12)cc34'\n",
    "    #target_mol1 ='NCCC4CCN(c3ccc(n2ccc1ccncc12)nc3)CC4'\n",
    "    #target_mol1 ='Cc5ccc(S(=O)(=O)OCOc4ccc3cc(n2ccc1ccccc12)ncc3c4)cc5'\n",
    "    #target_mol1 ='Cc5ccc(S(=O)(=O)OCCCOc4ccc3cc(n2ccc1ccncc12)ncc3c4)cc5'\n",
    "    \n",
    "    #target_mol1 ='FCCCOc4ccc3cc(n2ccc1ccncc12)ncc3c4'\n",
    "    \n",
    "    \n",
    "    #target_mol1 = 'CC(=O)c2ccc1cc(N(C)CCOCCF)ccc1c2' #M5\n",
    "    #target_mol1 = 'CC(c2ccc1cc(N(C)CCOCCF)ccc1c2)C(C#N)C#N' #M6\n",
    "    #target_mol1 = 'CC(=C(C#N)C#N)c2ccc1cc(N(C)CCOCCF)ccc1c2' #7\n",
    "    #target_mol1 = 'CC(=C(C#N)C#N)c3ccc2cc(N(C)CCOCCOS(=O)(=O)c1ccc(C)cc1)ccc2c3' #6\n",
    "    \n",
    "    #below is ABtest1-1\n",
    "    #target_mol1 ='[H][C@]12CCC[C@](CCCCC3=CC=C(OCOC)C=C3)(OC3=C1C(=O)CCC3)O2'\n",
    "    #below is a arbitrary test\n",
    "    #target_mol1 = '[H][C@@]12OC3=C(O)C=CC4=C3[C@@]11CCN(C)[C@]([H])(C4)[C@]1([H])C=C[C@@H]2O'\n",
    "    #below is alreay in base compounds\n",
    "    #target_mol1 = 'CC(=O)NC1=CC=C(O)C=C1'\n",
    "    #below is REMDESIVIR\n",
    "    target_mol1 = 'N#C[C@@]1(c2ccc3c(N)ncnn23)O[C@H](COCc2ccccc2)[C@@H](OCc2ccccc2)[C@H]1OCc1ccccc1'\n",
    "    #target_mol1 = 'CCC(CC)COC(=O)[C@H](C)NP(=O)(OC[C@H]1O[C@@](C#N)(c2ccc3c(N)ncnn23)[C@H](O)[C@@H]1O)Oc1ccccc1'\n",
    "    try:\n",
    "        target_mol = Chem.MolToSmiles(Chem.MolFromSmiles(target_mol1),allHsExplicit=0,allBondsExplicit=0, isomericSmiles=1)\n",
    "        if target_mol not in starting_mols: \n",
    "#            root = Node(state={target_mol})\n",
    "            path = plan(target_mol, expansion_net, filter_net, rollout_net, iterations=iters, max_depth=max_d)\n",
    "            path.sort(key = lambda x: x[-1], reverse = True)\n",
    "            if path:\n",
    "                pathset = defaultdict(list)\n",
    "                for elem in path:\n",
    "                    pathset[elem[-1]].append(elem) \n",
    "                    \n",
    "        else: print('target is alreay in base compounds')\n",
    "    except Exception as e:\n",
    "        print('error: {}'.format(e))\n",
    "        print('mol which is unable to standize: {}'.format(target_mol1))  \n",
    " \n",
    "    #    import ipdb; ipdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pathset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pathnot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8f20dba69d11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpathnot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pathnot' is not defined"
     ]
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "save_dir = os.path.join(os.getcwd(), 'images/','target_mol/')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for key,value in pathset.items():\n",
    "#    if key != 0.8732798339579952: break\n",
    "    ms=[]\n",
    "    ms.append(target_mol)\n",
    "    for a in range(len(value[0])-1):\n",
    "        value[0][a][1]\n",
    "        for b in value[0][a][1]:\n",
    "            if b not in ms:\n",
    "                ms.append(b)\n",
    "    msmol=[Chem.MolFromSmiles(mol) for mol in ms]\n",
    "    \n",
    "    img=Draw.MolsToGridImage(msmol[:],molsPerRow=3,subImgSize=(800,200))\n",
    "    img.save(save_dir+str(key)+'.png')  \n",
    "with open(save_dir+'pathset.pickle', 'wb') as f:\n",
    "    pickle.dump(pathset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/path-new.pickle', 'wb') as f:\n",
    "    pickle.dump(path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.9226209927010536: [[('([O;H0;+0:7]-[P;H0;+0:6].[cH;+0:1]:[cH;+0:2]:[cH;+0:3]:[cH;+0:4]:[cH;+0:5])>>([F;H0;+0]-[c;H0;+0:1]:[c;H0;+0:2](-[F;H0;+0]):[c;H0;+0:3](-[F;H0;+0]):[c;H0;+0:4](-[F;H0;+0]):[c;H0;+0:5]-[F;H0;+0].[P;H0;+0:6]-[O;H0;+0]-[c;H0;+0]1:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:1).([OH;+0:7])',\n",
       "                {'CCC(CC)COC(=O)[C@H](C)N[P@](=O)(Oc1ccccc1)Oc1c(F)c(F)c(F)c(F)c1F',\n",
       "                 'N#C[C@@]1(c2ccc3c(N)ncnn23)O[C@H](CO)[C@@H](O)[C@H]1O'}),\n",
       "               ('([OH;+0:1].[OH;+0:2].[OH;+0:3])>>([O;H0;+0:1]-[CH2;+0]-[c;H0;+0]1:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:1.[O;H0;+0:2]-[CH2;+0]-[c;H0;+0]1:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:1.[O;H0;+0:3]-[CH2;+0]-[c;H0;+0]1:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:[cH;+0]:1)',\n",
       "                {'CCC(CC)COC(=O)[C@H](C)N[P@](=O)(Oc1ccccc1)Oc1c(F)c(F)c(F)c(F)c1F',\n",
       "                 'N#C[C@@]1(c2ccc3c(N)ncnn23)O[C@H](COCc2ccccc2)[C@@H](OCc2ccccc2)[C@H]1OCc1ccccc1'}),\n",
       "               0.9226209927010536]]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
