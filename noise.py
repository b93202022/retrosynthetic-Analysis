#noise student
import pickle
import glob
from collections import defaultdict
import math
from tqdm import tqdm
from time import time
#import json
#import molvs
import random
#import policies

#from mcts import Node, mcts
import tensorflow as tf
from rdkit import Chem
from rdkit.Chem import AllChem

import os
import numpy as np

from highway_layer import Highway
#匯入深度學習的框架函式庫：keras
import keras
from keras import backend as K
from keras.initializers import Constant
from keras.utils import plot_model
#keras用以建立模型架構的函數
from keras.models import Sequential, load_model, Model

#keras中建立深度學習layer的函數

from keras.layers import Dense, Dropout, BatchNormalization, Activation, Multiply, Add, Lambda, Input

#keras訓練演算法函數
from keras import regularizers
from keras.optimizers import Adam

#keras提早判停的函數
from keras.callbacks import EarlyStopping, ModelCheckpoint

#it's hard to reproduce results, so close all seeds
#os.environ['PYTHONHASHSEED'] = '0'
#np.random.seed(0)
#tf.set_random_seed(0)
#random.seed(0)

#to solve problem:Blas GEMM launch failed
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
#config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
config.gpu_options.allocator_type = 'BFC' #A "Best-fit with coalescing" algorithm, simplified from a version of dlmalloc.
config.gpu_options.per_process_gpu_memory_fraction = 0.95
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config)) 


fp_dim = 16384
recfp_dim = 2048
batch_size=256

def fps_to_arr(fps):
    """Faster conversion to ndarray"""
    arrs = []
    for fp, info in zip(fps[0],fps[1]):
        onbits = list(fp.GetOnBits())
        arr = np.zeros(fp.GetNumBits())
        for onbit in onbits:
            arr[onbit] = len(info[onbit])
        arrs.append(arr)
    arrs = np.array(arrs)
    return arrs




def fingerprint_mols(mols, fp_dim):
    fps = []
    infos = []
    for mol in mols:
        mol = Chem.MolFromSmiles(mol)
        info={}
        # Necessary for fingerprinting
        # Chem.GetSymmSSSR(mol)

        # "When comparing the ECFP/FCFP fingerprints and
        # the Morgan fingerprints generated by the RDKit,
        # remember that the 4 in ECFP4 corresponds to the
        # diameter of the atom environments considered,
        # while the Morgan fingerprints take a radius parameter.
        # So the examples above, with radius=2, are roughly
        # equivalent to ECFP4 and FCFP4."
        # <http://www.rdkit.org/docs/GettingStartedInPython.html>
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=int(fp_dim), useChirality=1, bitInfo=info)
        # fold_factor = fp.GetNumBits()//fp_dim
        # fp = DataStructs.FoldFingerprint(fp, fold_factor)
        fps.append(fp)
        infos.append(info)
    return fps, infos

def preprocess(X, fp_dim):
    # Compute fingerprints
    FPs = fps_to_arr(fingerprint_mols(X, fp_dim))
    # Apply variance threshold
    # return np.log(X[:,self.idx] + 1) 
    #FPs = np.log(dataX[:,idx]+1)
#    FPs = np.log(dataX+1)
    return FPs
def smi_list_from_str(inchis):
    '''string separated by ++ to list of RDKit molecules'''
    return [inchi.strip() for inchi in inchis.split('++')]

def fold(x):
    z=tf.subtract(x[0], x[1])
#    z_shape=tf.Tensor.shape(z)

#    z_shape=z.get_shape().as_list()
    zv=tf.reshape(z,[-1,8,2048])
    return tf.reduce_sum(zv, 1) 

def cosine(x):
    prod_net = x[0]
    react_net = x[1]
#    prod_norm = tf.nn.l2_normalize(prod_net, axis=-1)
#    react_norm = tf.nn.l2_normalize(react_net, axis=-1)
    cosine_sim = tf.reduce_sum(tf.multiply(prod_net, react_net), axis=-1,keepdims=True)
#    cosine_sim = tf.squeeze(cosine_sim,[1])
#    return tf.nn.sigmoid(cosine_sim)
    return tf.nn.sigmoid(cosine_sim)
# get average auc between different batches over the epoch, so don't use. otherwise validation process  always get wrong results
def auc2(y_true, y_pred):
    auc = tf.metrics.auc(y_true, y_pred)[1]
    K.get_session().run(tf.local_variables_initializer())
    return auc

# AUC for a binary classifier, this AUC is a little underestimated due to minimum areas.
def auc1(y_true, y_pred):
    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)
    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)
    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)
    binSizes = -(pfas[1:]-pfas[:-1])
    s = ptas*binSizes
    return K.sum(s, axis=0)
#-----------------------------------------------------------------------------------------------------------------------------------------------------
# PFA, prob false alert for binary classifier(FPR)
def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):
    y_pred = K.cast(y_pred >= threshold, 'float32')
    # N = total number of negative labels
    N = K.sum(1 - y_true)
    # FP = total number of false alerts, alerts from the negative class labels
    FP = K.sum(y_pred - y_pred * y_true)
    return FP/N
#-----------------------------------------------------------------------------------------------------------------------------------------------------
# P_TA prob true alerts for binary classifier(TPR)
def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):
    y_pred = K.cast(y_pred >= threshold, 'float32')
    # P = total number of positive labels
    P = K.sum(y_true)
    # TP = total number of correct alerts, alerts from the positive class labels
    TP = K.sum(y_pred * y_true)
    return TP/P
# PFA, prob false alert for binary classifier(FPR)
def FPR(y_true, y_pred):
    y_pred = K.cast(y_pred >= 0.5, 'float32')
    # N = total number of negative labels
    N = K.sum(1 - y_true)
    # FP = total number of false alerts, alerts from the negative class labels
    FP = K.sum(y_pred - y_pred * y_true)
    return FP/N
#-----------------------------------------------------------------------------------------------------------------------------------------------------
# P_TA prob true alerts for binary classifier(TPR)
def TPR(y_true, y_pred):
    y_pred = K.cast(y_pred >= 0.5, 'float32')
    # P = total number of positive labels
    P = K.sum(y_true)
    # TP = total number of correct alerts, alerts from the positive class labels
    TP = K.sum(y_pred * y_true)
    return TP/P

# ACC= (TP + TN) / (P + N)
def ACCR(y_true, y_pred):
    y_pred = K.cast(y_pred >= 0.9, 'float32')
    # P = total number of positive labels
    P = K.sum(y_true)
    # N = total number of negative labels
    N = K.sum(1 - y_true)    
    # TP = total number of correct alerts, alerts from the positive class labels
    TP = K.sum(y_pred * y_true)
    # TN = total number of correct alerts, alerts from the negtive class labels
    TN = K.sum((1-y_pred) * (1-y_true))    
    return (TP+TN)/(P+N)

class DataGenerator(keras.utils.Sequence):
    
    def __init__(self, X, y, batch_size=1, fp_dim=16384, recfp_dim=2048):
        self.batch_size = batch_size
        self.X = X
        self.y = y
        
        self.indexes = np.arange(len(self.X))
        
        self.fp_dim = fp_dim
        self.recfp_dim = recfp_dim

    def __len__(self):
        #计算每一个epoch的迭代次数
        return int(np.ceil(len(self.X) / int(self.batch_size)))

    def __getitem__(self, index):
        #生成每个batch数据，这里就根据自己对数据的读取方式进行发挥了
        # 生成batch_size个索引
        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        # 根据索引获取datas集合中的数据
        batch_datasX = [self.X[k] for k in batch_indexs]
        batch_datasy = [self.y[k] for k in batch_indexs]
        
        # 生成数据
        X = preprocess(batch_datasX, self.fp_dim)
        y = np.zeros((len(batch_datasy),self.recfp_dim))
        for i,a in enumerate(batch_datasy):
            n = np.zeros((1,self.recfp_dim))
            for b in smi_list_from_str(a):
                n += preprocess([b], self.recfp_dim)
            p = X[i].reshape((-1,self.recfp_dim))    
            y[i] = np.sum(p, 0, keepdims=True)- n
        
#        y = y.astype(np.int64)
        return [X, y]

if __name__ == '__main__':
    print('Loading data...')

    
    tem_simp = set()
    tem_val = set()
    tem_val_pos = []
    tem_val_neg = []    
    prods = []
    reacs = []
    labels = []
    count=0
    countp =-1
    countn =-1

    save_dir = os.path.join(os.getcwd(), 'saved_models')
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)

    model_path_i = os.path.join(save_dir, 'trained_model_inscope_all-test0') 
    threshold = 0.5

    filter_net =  load_model(model_path_i, custom_objects={'ACCR': ACCR,'auc2': auc2,'auc1': auc1,'TPR': TPR, 'FPR': FPR,'Highway': Highway,'fold': fold,'cosine': cosine, 'tf': tf})

    '''
    with open('data/inscopedata.dat', 'r') as f:
        for l in tqdm(f, desc='inscopedata'):
            
            tem_simp.add(l.strip())
            if l.strip().split('\t')[2] =='1':
                tem_val_pos.append(l.strip())
            if l.strip().split('\t')[2] =='0':
                tem_val_neg.append(l.strip()) 
    for l in tem_val_pos:
        countp +=1
        if countp%10 ==0:
            tem_val.append(l)
    for l in tem_val_neg:
        countn +=1
        if countn%2 ==1:
            tem_val.append(l)    
    '''
    #'''
    with open('data/inscopedata_rev.dat', 'r') as f:
        for l in tqdm(f, desc='inscopedata'):
            
            tem_simp.add(l.strip())
            tem_val.add(l.strip())

    with open('data/inscopedata2.dat', 'r') as f:
        for l in tqdm(f, desc='inscopedata'):
            tem_simp.add(l.strip())

    #with open('data/inscopedata4.dat', 'r') as f:
    #    for l in tqdm(f, desc='inscopedata'):
    #        tem_simp.add(l.strip())             
    #'''
    tem_simp.difference_update(tem_val)
        
    for ll in tem_simp:        
        prod,reac,label = ll.split('\t') 
        if label !='0':
            print('errors')
#            continue
        
        prods.append(prod)
        reacs.append(reac)
        
    training_generator = DataGenerator(X=prods, y=reacs, batch_size=batch_size, fp_dim=fp_dim, recfp_dim=recfp_dim)   
        # Predict applicable rules
    #predict_i = filter_net.predict_on_batch([X, y])
    predict_i = filter_net.predict_generator(generator=training_generator, 
                                            max_queue_size=4, 
                                            workers=3, 
                                            use_multiprocessing=1, 
                                            verbose=1)
    for pred in predict_i:
        label = str(float(pred))
        #if pred >= threshold:
        #    label = '1' 
             
        #    count+=1
        #else: 
        #    label = '0'
        labels.append(label)

    print('positive data ratio:',count/len(labels))
    print('total data:',len(labels))
    transforms=list(zip(prods,reacs,labels))
    transforms.sort()
    with open('data/inscopedata_rev_noise.dat', 'w') as f:
        f.write('\n'.join(['\t'.join(rxn_prod) for rxn_prod in transforms]))